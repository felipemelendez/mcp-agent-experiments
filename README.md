# ğŸ› ï¸ MCP Quickstart

**Connect any LLM to any MCP serverâ€”no proprietary client required**

Model Context Protocol (MCP) is a lightweight spec that lets a largeâ€‘languageâ€‘model reach *outside* its training data by calling remote â€œserversâ€ that expose tools such as browser automation, database queries, 3â€‘D renderers, and more.

`mcpâ€‘use` is a Python SDK *and* agent framework that hands those tools to your chat model without ceremony:

* **Single config file** â€“ list every MCP server you want and how to start or attach to it.
* **MCPClient** â€“ spins up the servers, pulls each toolâ€™s JSON schema, and maintains live sessions.
* **MCPAgent** â€“ wraps your LLM (GPTâ€‘4o, Claude, LlamaÂ 3 â€¦) and runs the classicÂ *think â†’ act â†’ observe* loop for you.
* Batteries included â€“ multiâ€‘server routing, disallowed\_tools safety filter, and dropâ€‘in LangChain compatibility.

---

## ğŸ¤– How an Agent Thinks

1. **Think** â€“ The model receives the user prompt along with a scratch-pad: a running history of its previous thoughts, tool calls, and observations. This gives the model memory within the loop, helping it reason step-by-step, avoid repetition, and stay grounded in context.
2. **Act** â€“ If the model decides to call a tool, MCPAgent encodes the request and executes it via MCPClient.
3. **Observe** â€“ The result of the tool call is captured and appended to the scratch-pad.
4. **Repeat** - The model receives the updated context and continues reasoning until it outputs a final answer (i.e., regular text) or hits a termination condition.

> This loop turns a one-shot chat model into a goal-seeking problem-solverâ€”able to gather live data, perform multi-step reasoning, and take real-world actions.

### Under the Hood
What happens inside `await agent.run("â€¦")`
```
User prompt
   â”‚
   â–¼
[LLM â€œthinksâ€] â”€â”€â–º decides to call `search_web`
   â”‚                          â”‚
   â”‚               MCPAgent marshals JSON
   â”‚                          â–¼
[MCPClient] â”€â”€â–º finds matching MCP server â”€â”€â–º tool runs in a sandbox
   â”‚                                               â”‚
   â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€ JSON result comes back â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â–¼
Tool output appended to chat; loop continues
```
The loop ends when the model returns ordinary text instead of another tool-call, and that becomes the final answer.

---

## ğŸš€ Quickstart Checklist (from zero)

### 1Â Â Start in a fresh terminal

```
mkdir mcpâ€‘quickstart
cd mcpâ€‘quickstart
```

### 2Â Â Open the folder in Cursor (optional)

```
open -a "Cursor" .
```

### 3Â Â Create & activate the virtual env *(repeat inside any new shell)*

```
python3 -m venv venv
source venv/bin/activate
```

### 4Â Â Install dependencies

```
pip install mcpâ€‘use langchainâ€‘openai pythonâ€‘dotenv
```

*Need Anthropic, Groq, etc.?* Swap in `langchainâ€‘anthropic`, `langchainâ€‘groq`, â€¦

### 5Â Â Add your LLM API key

```
echo "OPENAI_API_KEY=skâ€‘..." > .env
```

### 6Â Â Create **browser\_mcp.json**

```
{
  "mcpServers": {
    "playwright": {
      "command": "npx",
      "args": ["@playwright/mcp@latest"],
      "env": { "DISPLAY": ":1" }
    }
  }
}
```

### 7Â Â Create **agent.py**

```
import asyncio, os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from mcp_use import MCPClient, MCPAgent

load_dotenv()

async def main():
    client = MCPClient.from_config_file("browser_mcp.json")
    llm    = ChatOpenAI(model="gptâ€‘4o")
    agent  = MCPAgent(llm=llm, client=client, max_steps=30)
    result = await agent.run("Find the best coffee shop in Brooklyn")
    print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

### 8Â Â Run the agent

```
python agent.py
```

âŒ›Â First launch downloads the Playwright MCP server viaÂ *npx*, so it may take a minute.


Project tree should now look like:

```
mcpâ€‘quickstart/
â”œâ”€â”€ agent.py
â”œâ”€â”€ browser_mcp.json
â”œâ”€â”€ .env
â””â”€â”€ venv/
```

---

## ğŸ“¦ Required Packages & Common Commands

* `pip install mcpâ€‘use` â€“ unified MCP client library.
* `pip install "mcpâ€‘use[search]"` â€“ pulls `fastembed` & tiny data files when you need semantic search tools.
* `pip install langchainâ€‘openai` â€“ LangChain wrapper for OpenAI models.
* `python3 -m venv venv` â€“ create *venv/*.
* `source venv/bin/activate` â€“ enter the venv in the current shell.
* `deactivate` â€“ leave the venv (closing the terminal also resets envâ€‘vars).

---

## ğŸ§© Example Agents Included in This Repo

| Agent                    | Purpose                                                                                               | MCP Servers Used                        |
| ------------------------ | ------------------------------------------------------------------------------------------------------ | --------------------------------------- |
| **Quick Restaurant Finder** | One-liner demo that uses a Playwright browser to fetch â€œbest restaurantâ€ results for San Francisco | Playwright (browser automation)         |
| **Airbnb Finder**        | Surfaces top rentals that meet price, amenity, and date constraints                            | Playwright (browser) + Airbnb MCP       |
| **Restaurant Scout**     | Ranks the best restaurants in any city via live Google search and review-site scraping                 | Playwright (browser automation)         |
| **Equity Screener**      | Scrapes live fundamentals and returns growth/value/dividend shortlists on demand                       | Playwright (browser automation)         |


---

## ğŸ’¡ Tips & Troubleshooting

* **venv gotcha** â€“ opening a new terminal (or the builtâ€‘in one inside Cursor) starts a fresh shell, so reâ€‘activate the venv.
* **ModuleNotFoundError: fastembed** â€“ install with `pip install "mcpâ€‘use[search]"` or `pip install fastembed`.
* **Step limit** â€“ raise `max_steps` if your agent stops too early.
* **Safety filter** â€“ block risky tools by passing `disallowed_tools=[...]` to `MCPAgent`.

---

## âš–ï¸ License

MIT â€“ see LICENSE for details.
