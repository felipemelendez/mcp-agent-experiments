# ğŸ› ï¸ MCP Quickstart

> MCP lets any LLM use realâ€‘world tools in 3Â lines of codeâ€”from browsing the web to querying databases, all with an open protocol.

**Connect any LLM to any MCP server â€” no proprietary client required**

Model Context Protocol (MCP) is a lightweight spec that lets large language models reach *outside* their training data by calling remote servers that expose tools such as browser automation, database queries, 3â€‘D renderers, and more.

---

## ğŸ“‘ Table of Contents
- [ğŸš€ Why MCP Matters](#why-mcp-matters)
- [ğŸ¤– Whatâ€™s an Agent?](#whats-an-agent)
- [âš¡ Quickstart Checklist](#quickstart-checklist-from-zero)
- [ğŸ§° Why We Use mcp-use](#why-we-use-mcp-use)
- [ğŸ§© Example Agents](#example-agents)
- [ğŸ’¡ Tips & Troubleshooting](#tips--troubleshooting)
- [ğŸ“š Common Commands](#common-commands)
- [ğŸŒ Appendix â€” Protocols Primer](#appendix-understanding-protocols)
- [ğŸ“¡ Appendix â€” MCP Protocol Details](#appendix-mcp-protocol-details)
- [âš–ï¸ License](#license)


---

## ğŸš€ Why MCP Matters

*MCP is â€œREST for AI toolsâ€ â€” where REST standardized **data exchange**, MCP standardizes **capability exchange**.*

Just as HTTP and REST transformed web development, MCP creates a standard way for AI models to discover and use realâ€‘world tools:

1. **Vendorâ€‘agnostic** â€“ Any JSONâ€‘emitting LLM can drive any MCP server  
2. **SafetyÂ &Â control** â€“ Tool schemas make capabilities explicit; risky actions can be whitelisted or sandboxed  
3. **Composability** â€“ Mixâ€‘andâ€‘match servers (browserÂ +Â vectorÂ DBÂ +Â email) just as REST let us compose microâ€‘services  

---

## ğŸ¤– What's an Agent?

*An agent is a superâ€‘smart assistant that can reason, choose a tool, and use it on your behalf.*

### The Agent Loop
1. **Listen** â€” reads your goal in plain language  
2. **Plan** â€” decides whether the answer needs outside information or action  
3. **Act** â€” if needed, picks the best tool (search engine, database, emailâ€‘sender, etc.) and calls it with the right inputs  
4. **Learn** â€” looks at the tool's reply and figures out what to do next  
5. **Respond** â€” once it has everything, explains the final answer back to you  

> ğŸ“ **Analogy:**  
> Imagine asking a human assistant, â€œBook me a hotel in Paris underÂ $200Â aÂ night.â€  
> *They think* (compare options) â†’ *use a browser* (look up hotels) â†’ *read results* (price, location) â†’ *decide* (which fits) â†’ *reply* (the choice).  
> An MCP agent does the same loop, but lightningâ€‘fast and with computer tools.

### ğŸ› ï¸ Behind the Scenes (but simplified)

| Step | Underâ€‘theâ€‘hood action |
|------|----------------------|
| **Think** | The language model brainstorms possible next moves |
| **Toolâ€‘Call** | It sends a short, structured request (JSON) to the chosen tool |
| **Observe** | Gets structured results back (numbers, URLs, text) |
| **RepeatÂ orÂ Finish** | Loops until it's confident enough to answer you |

No coding is required from youâ€”the agent and MCP handle all the â€œplumbing,â€ so your only job is to *ask the question*.

---

## âš¡ Quickstart Checklist (from zero)

### 1Â Â Start in a fresh terminal
```bash
mkdir mcp-quickstart
cd mcp-quickstart
```

### 2Â Â Open the folder in CursorÂ *(optional)*
```bash
open -a "Cursor" .
```

### 3Â Â CreateÂ & activate the virtualÂ env *(repeat inside any new shell)*
```bash
python3 -m venv venv
source venv/bin/activate
```

### 4Â Â Install dependencies
```bash
pip install mcp-use langchain-openai python-dotenv
```
*Need Anthropic, Groq, etc.?* Swap in `langchain-anthropic`, `langchain-groq`,Â â€¦

### 5Â Â Add your LLM API key in aÂ .env file
```bash
echo "OPENAI_API_KEY=sk-..." > .env
```

### 6Â Â Create **browser_mcp.json**
```json
{
  "mcpServers": {
    "playwright": {
      "command": "npx",
      "args": ["@playwright/mcp@latest"],
      "env": { "DISPLAY": ":1" }
    }
  }
}
```

### 7Â Â Create **agent.py**
```python
import asyncio, os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from mcp_use import MCPClient, MCPAgent

load_dotenv()

async def main():
    client = MCPClient.from_config_file("browser_mcp.json")
    llm    = ChatOpenAI(model="gpt-4o")
    agent  = MCPAgent(llm=llm, client=client, max_steps=30)
    result = await agent.run("Find the best coffee shop in Brooklyn")
    print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

### 8Â Â Run the agent
```bash
python agent.py
```
âŒ›Â First launch downloads the Playwright MCP server via **npx**, so it may take a minute.

Project tree should now look like:
```text
mcp-quickstart/
â”œâ”€â”€ agent.py
â”œâ”€â”€ browser_mcp.json
â”œâ”€â”€ .env
â””â”€â”€ venv/
```

---

## ğŸ§° Why We Use mcp-use

*A tiny Python toolkit that erases the boilerplate you'd normally write to talk to MCP servers.*

`mcp-use` is a plugâ€‘andâ€‘play **agent harness** that:

1. **BootsÂ orÂ attaches** to every MCP server you list (Playwright, Postgres, custom tools,Â â€¦)  
2. **Fetches each tool's JSON schema** so the LLM sees a live catalog of capabilities  
3. **Keeps sessions alive**, meaning heavyweight resources (an open browser, a DB connection) persist across calls  
4. Exposes two highâ€‘level classes:  
   * **MCPClient** â€“ manages connections and routes calls  
   * **MCPAgent** â€“ wraps your LLM and runs the classic *thinkÂ â†’Â actÂ â†’Â observe* loop for you  

### ğŸš€ Why It Feels â€œServerlessâ€

Everything runs **right inside your terminal** (or any Python script). Thereâ€™s **no separate daemon** or hosted gatewayâ€”your process speaks directly to MCP servers over WebSocket/stdio.

```python
async def main():
    client = MCPClient.from_config_file("browser_mcp.json")
    llm    = ChatOpenAI(model="gpt-4o")
    agent  = MCPAgent(llm=llm, client=client)
    result = await agent.run("Find me ramen in Portland under $20")
    print(result)
```

### ğŸ†š How It Differs (in one breath)

Unlike heavyweight â€œagent platformsâ€ that ship their *own* orchestration layer, **mcp-use stays minimal**: no extra protocol, no hosted runtime, no vendor lockâ€‘inâ€”just a thin Python wrapper around the open MCP spec.

---

## ğŸ§© Example Agents

| Agent | Purpose | MCPÂ ServersÂ Used |
| ----- | ------- | ---------------- |
| **Quick Restaurant Finder** | Oneâ€‘liner demo that uses a Playwright browser to fetch â€œbest restaurantâ€ results for SanÂ Francisco | Playwright (browser automation) |
| **Airbnb Finder** | Surfaces top holiday rentals that meet price, amenity, and date constraints | Playwright (browser) + Airbnb MCP |
| **Restaurant Scout** | Ranks the best restaurants in any city via live Google search and reviewâ€‘site scraping | Playwright (browser automation) |
| **Equity Screener** | Scrapes live fundamentals and returns growth/value/dividend shortlists on demand | Playwright (browser automation) |

---

## ğŸ’¡ Tips & Troubleshooting

* **venv gotcha** â€“ Opening a new terminal (or the builtâ€‘in one inside Cursor) starts a fresh shell, so reâ€‘activate the venv with `source venv/bin/activate`. (Closing your terminal without deactivating does not break anythingâ€”just reactivate next time.)  
* **ModuleNotFoundError: fastembed** â€“ Install with `pip install "mcp-use[search]"` or `pip install fastembed`  
* **Step limit** â€“ Raise `max_steps` if your agent stops too early  
* **Safety filter** â€“ Block risky tools by passing `disallowed_tools=[...]` to **MCPAgent**  

---

## ğŸ“š Common Commands

* `pip install mcp-use` â€“ unified MCP client library  
* `pip install "mcp-use[search]"` â€“ pulls fastembedÂ & tiny data files when you need semantic search tools  
* `pip install langchain-openai` â€“ LangChain wrapper for OpenAI models  
* `python3 -m venv venv` â€“ create **venv/**  
* `source venv/bin/activate` â€“ enter the venv in the current shell  
* `deactivate` â€“ leave the venv (closing the terminal also resets envâ€‘vars)  

---

## ğŸŒ Appendix: Understanding Protocols

*A 90â€‘second refresher on why standard handshakes matter, from web pages to AI tools.*

### ğŸ“„ HTTP â€” the web's means of communication
* **What it is:** The **H**yper**T**ext **T**ransfer **P**rotocol  
* **What it solved:** Early browsers and servers spoke different dialects. HTTP standardized the rulesâ€”verbs such as **GET** and **POST**, status codes like **200Â OK** or **404Â NotÂ Found**, headers that describe the payloadâ€”so *any* browser could fetch *any* page from *any* server  
* **Why it matters:** One common contract let developers build on a stable foundation instead of reinventing networking for every site  

### ğŸ”„ RESTful APIs â€” a shared playbook for data
* **What it is:** Conventions layered *on top of* HTTP that map CRUD actions to HTTP verbs:  
  * **GET** â†’ read  
  * **POST** â†’ create  
  * **PUT/PATCH** â†’ update  
  * **DELETE** â†’ remove  
* **What it solved:** Web, mobile, and IoT apps could exchange structured data the *same predictable way*, unlocking speed and interoperability  

### ğŸ¤– MCP â€” a protocol for agent superâ€‘powers
* **What it is:** **M**odel **C**ontext **P**rotocol, a spec that standardizes how largeâ€‘languageâ€‘model (LLM) *agents* discover, call, and coordinate external toolsâ€”from a headless browser to a SQL database or 3â€‘D renderer  
* **What it solves:**  
  1. **Discovery** â€“ Tools publish selfâ€‘describing JSON schemas, so theÂ agent knows exactly which functions exist and what arguments they expect  
  2. **Transport** â€“ A single biâ€‘directional channel (WebSocket or HTTPâ€‘stream) carries both requests and responses, eliminating custom glue code  
  3. **SessionÂ state** â€“ MCP keeps longâ€‘running resources (e.g., an open browser tab) alive across calls, letting the LLM *thinkÂ â†’Â actÂ â†’Â observe* without constant reâ€‘initialization  

---

## ğŸ“¡ Appendix: MCP Protocol Details

*For developers who want to understand the protocol implementation details.*

### Transport & Envelope

| Layer | Technology | Why It's Used |
|-------|------------|---------------|
| **Wire** | JSONâ€‘RPCÂ 2.0 over **WebSocket**, **stdio**, or **SSE** | Gives request/response IDs, typed errors, and streaming without reinventing the wheel |
| **Session** | *Stateful* connection negotiated via initialize | Keeps resources (e.g., a browser tab) alive across many calls |

> **Handshake in two messages (clientÂ â†’Â server):**  
> 1ï¸âƒ£Â `initialize` â€” declares client name, version, supported transports  
> 2ï¸âƒ£Â `initialize/complete` â† server returns metadataÂ + advertised features (tools, resources, prompts)  

### Tool Discovery
```json
// list available tools
{
  "jsonrpc": "2.0",
  "id": 42,
  "method": "tools/list"
}

// server response (truncated)
{
  "jsonrpc": "2.0",
  "id": 42,
  "result": {
    "tools": [{
      "name": "calculate_sum",
      "description": "Add two numbers together",
      "inputSchema": {
        "type": "object",
        "properties": {
          "a": { "type": "number" },
          "b": { "type": "number" }
        },
        "required": ["a", "b"]
      },
      "annotations": {
        "idempotentHint": true,
        "openWorldHint": false
      }
    }]
  }
}
```

### Tool Invocation
```json
// model decides to add 5 + 7
{
  "jsonrpc": "2.0",
  "id": 43,
  "method": "tools/call",
  "params": {
    "name": "calculate_sum",
    "arguments": { "a": 5, "b": 7 }
  }
},
{
  "jsonrpc": "2.0",
  "id": 43,
  "result": {
    "content": [{
      "type": "text",
      "text": "12"
    }]
  }
}
```
*The LLM reads â€œcontent,â€ sees â€œ12,â€ and either continues planning or prints the final answer.*

### Call Flow Cheatâ€‘Sheet
```mermaid
sequenceDiagram
  participant Host as LLM Host
  participant Client as MCP Client
  participant Server as MCP Server

  Host->>Client: initialize
  Client->>Server: initialize
  Server-->>Client: initialize/complete
  Client-->>Host: initialize/complete

  Host->>Client: tools/list
  Client->>Server: tools/list
  Server-->>Client: list result
  Client-->>Host: list result

  Host->>Client: tools/call (name, args)
  Client->>Server: tools/call
  Server-->>Client: result
  Client-->>Host: result
```

### Quick Recap
* **JSONâ€‘RPCÂ 2.0** keeps the wire format deadâ€‘simple  
* **Tool schemas** make capabilities explicitÂ & typeâ€‘safe  
* **Persistent sessions** avoid reâ€‘spinning heavy resources  
* **Uniform prefixes** (`tools/`, `resources/`, `prompts/`) keep the mental model flat  

With these pieces, you can crack open any MCP message log and know exactly what's happeningâ€”and, more importantly, *why*.

---

## âš–ï¸ License

MIT â€“ see LICENSE for details
